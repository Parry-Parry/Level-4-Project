{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Seq2Seq Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import datasets \n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "def tokenize_function(set):\n",
    "    inputs = tokenizer(set[\"code\"], max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "       labels = tokenizer(set[\"docstring\"], max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0ac7eaced0986bb3\n",
      "Reusing dataset json (C:\\Users\\Parry\\.cache\\huggingface\\datasets\\json\\default-0ac7eaced0986bb3\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|██████████| 1/1 [00:00<00:00, 148.15it/s]\n",
      "100%|██████████| 25873/25873 [00:52<00:00, 496.82ex/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = datasets.load_dataset('json', data_files=\"D:\\\\PROJECT\\\\data\\\\CodeSearchNet\\\\pyjava\\\\test.jsonl\")[\"train\"]\n",
    "tokenized_test = test_data.map(tokenize_function, remove_columns=test_data.column_names)\n",
    "test_set = tokenized_test.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Augmentation of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTest(model):\n",
    "    bleu = datasets.load_metric('bleu')\n",
    "    rouge = datasets.load_metric('rouge')\n",
    "    meteor = datasets.load_metric('meteor')\n",
    "\n",
    "    for input, reference in test_set:\n",
    "        model_pred = model(input)\n",
    "        bleu.add(predictions=model_pred, references=reference)\n",
    "        rouge.add(predictions=model_pred, references=reference)\n",
    "        meteor.add(predictions=model_pred, references=reference)\n",
    "    \n",
    "    return  {'bleu' : bleu.compute(), 'rouge' : rouge.compute(), 'meteor' : meteor.compute()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_path = \"D:\\\\PROJECT\\Models\\\\tiny\\\\model_trained\\\\checkpoint-310000\"\n",
    "small_path = \"\"\n",
    "medium_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_model = AutoModelForSeq2SeqLM.from_pretrained(tiny_path, \n",
    "    pad_token_id=1, \n",
    "    bos_token_id = 0, \n",
    "    eos_token_id = 2, \n",
    "    decoder_start_token_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = AutoModelForSeq2SeqLM.from_pretrained(small_path, \n",
    "    pad_token_id=1, \n",
    "    bos_token_id = 0, \n",
    "    eos_token_id = 2, \n",
    "    decoder_start_token_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_model = AutoModelForSeq2SeqLM.from_pretrained(medium_path, \n",
    "    pad_token_id=1, \n",
    "    bos_token_id = 0, \n",
    "    eos_token_id = 2, \n",
    "    decoder_start_token_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Parry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Parry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Parry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\PROJECT\\Level-4-Project\\notebooks\\evaluation.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000011?line=0'>1</a>\u001b[0m tiny_res \u001b[39m=\u001b[39m runTest(tiny_model)\n",
      "\u001b[1;32md:\\PROJECT\\Level-4-Project\\notebooks\\evaluation.ipynb Cell 7'\u001b[0m in \u001b[0;36mrunTest\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000006?line=3'>4</a>\u001b[0m meteor \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mload_metric(\u001b[39m'\u001b[39m\u001b[39mmeteor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000006?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m, reference \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(test_set[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m], test_set[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000006?line=6'>7</a>\u001b[0m     model_pred \u001b[39m=\u001b[39m model(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000006?line=7'>8</a>\u001b[0m     bleu\u001b[39m.\u001b[39madd(predictions\u001b[39m=\u001b[39mmodel_pred, references\u001b[39m=\u001b[39mreference)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000006?line=8'>9</a>\u001b[0m     rouge\u001b[39m.\u001b[39madd(predictions\u001b[39m=\u001b[39mmodel_pred, references\u001b[39m=\u001b[39mreference)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:489\u001b[0m, in \u001b[0;36mEncoderDecoderModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=483'>484</a>\u001b[0m kwargs_decoder \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=484'>485</a>\u001b[0m     argument[\u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdecoder_\u001b[39m\u001b[39m\"\u001b[39m) :]: value \u001b[39mfor\u001b[39;00m argument, value \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m argument\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mdecoder_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=485'>486</a>\u001b[0m }\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=487'>488</a>\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=488'>489</a>\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=489'>490</a>\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=490'>491</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=491'>492</a>\u001b[0m         inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=492'>493</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=493'>494</a>\u001b[0m         output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=494'>495</a>\u001b[0m         return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=495'>496</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_encoder,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=496'>497</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=498'>499</a>\u001b[0m encoder_hidden_states \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=500'>501</a>\u001b[0m \u001b[39m# optionally project encoder_hidden_states\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:798\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=795'>796</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=796'>797</a>\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=797'>798</a>\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39;49msize()\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=798'>799</a>\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=799'>800</a>\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "tiny_res = runTest(tiny_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "043b8ec81c094910c2b30aa6da4b87ee4064e49c6c2745b34a15c1b8c91b4ed0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('build')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
