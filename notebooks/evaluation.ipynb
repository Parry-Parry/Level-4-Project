{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Seq2Seq Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, LogitsProcessorList, MinLengthLogitsProcessor, TopKLogitsWarper, TemperatureLogitsWarper, BeamSearchScorer\n",
    "import torch\n",
    "import datasets \n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "def tokenize_function(set):\n",
    "    inputs = tokenizer(set[\"code\"], max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "       labels = tokenizer(set[\"docstring\"], max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Parry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Parry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Parry\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "bleu = datasets.load_metric('sacrebleu')\n",
    "rouge = datasets.load_metric('rouge')\n",
    "meteor = datasets.load_metric('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class testWrapper():\n",
    "    def __init__(self, model):\n",
    "        self.model = model.cuda()\n",
    "\n",
    "        self.beam_scorer = BeamSearchScorer(\n",
    "        batch_size=4,\n",
    "        max_length=self.model.config.max_length,\n",
    "        num_beams=4,\n",
    "        device=self.model.device,\n",
    "        )\n",
    "        \n",
    "        self.logits_processor = LogitsProcessorList(\n",
    "        [MinLengthLogitsProcessor(5, eos_token_id=self.model.config.eos_token_id)]\n",
    "        )\n",
    "\n",
    "        self.logits_warper = LogitsProcessorList(\n",
    "            [\n",
    "            TopKLogitsWarper(50),\n",
    "            TemperatureLogitsWarper(0.7),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        input_ids = torch.ones((4, 1), device=self.model.device, dtype=torch.long)\n",
    "        self.input_ids = input_ids * self.model.config.decoder_start_token_id\n",
    "    \n",
    "    def generate_string(self, batch):\n",
    "        inputs = tokenizer(batch[\"code\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        input_ids = inputs.input_ids.cuda()\n",
    "        attention_mask = inputs.attention_mask.cuda()\n",
    "        outputs = self.model.generate(input_ids, attention_mask=attention_mask)\n",
    "        output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        batch[\"pred_string\"] = output_str\n",
    "\n",
    "        predictions = output_str\n",
    "        references = [batch[\"docstring\"]]\n",
    "\n",
    "        rouge_output = rouge.compute(predictions=predictions, references=references, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "        bleu_output = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "        meteor_output = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "        batch[\"rouge2_precision\"] = round(rouge_output.precision, 4)\n",
    "        batch[\"rouge2_recall\"] = round(rouge_output.recall, 4)\n",
    "        batch[\"rouge2_fmeasure\"] = round(rouge_output.fmeasure, 4)\n",
    "        batch[\"bleu_score\"] = bleu_output[\"score\"]\n",
    "        batch[\"meteor_score\"] = meteor_output[\"meteor\"]\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    def test_gen(self, batch):\n",
    "        encoder_input_ids = tokenizer(batch['code'], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\").input_ids\n",
    "        model_kwargs = {\n",
    "        \"encoder_outputs\": self.model.get_encoder()(\n",
    "            encoder_input_ids.repeat_interleave(4, dim=0), return_dict=True\n",
    "            )\n",
    "        }   \n",
    "        outputs = self.model.beam_sample(\n",
    "        self.input_ids, self.beam_scorer, logits_processor=self.logits_processor, logits_warper=self.logits_warper, **model_kwargs\n",
    "        )\n",
    "        batch['pred_string'] = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_compute(results):\n",
    "    predictions=results[\"pred_string\"] \n",
    "    references=results[\"docstring\"]\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=predictions, references=references, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "    bleu_output = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    meteor_output = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "        \"bleu_score\" : bleu_output[\"score\"],\n",
    "        \"meteor_score\" : meteor_output[\"meteor\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelSetup(path):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(path)\n",
    "\n",
    "    model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "    model.config.eos_token_id = tokenizer.sep_token_id\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.vocab_size = model.config.encoder.vocab_size\n",
    "    model.config.num_beams = 4\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3f4bdb1ea0486ba6\n",
      "Reusing dataset json (C:\\Users\\Parry\\.cache\\huggingface\\datasets\\json\\default-3f4bdb1ea0486ba6\\0.0.0\\ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n",
      "100%|██████████| 1/1 [00:00<00:00, 250.18it/s]\n"
     ]
    }
   ],
   "source": [
    "test_set = datasets.load_dataset('json', data_files=\"D:\\\\PROJECT\\\\data\\\\CodeSearchNet\\\\py_clean\\\\test.jsonl\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_test = test_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...</td>\n",
       "      <td>str-&gt;list\\n    Convert XML to URL List.\\n    F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def dailymotion_download(url, output_dir='.', ...</td>\n",
       "      <td>Downloads Dailymotion videos by URL.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def sina_download(url, output_dir='.', merge=T...</td>\n",
       "      <td>Downloads Sina videos by URL.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def sprint(text, *colors):\\n    \"\"\"Format text...</td>\n",
       "      <td>Format text with color or other effects into A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def print_log(text, *colors):\\n    \"\"\"Print a ...</td>\n",
       "      <td>Print a log message to standard error.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14913</th>\n",
       "      <td>def from_grayscale(im, channels_on=(True, True...</td>\n",
       "      <td>Return a canvas from a grayscale image.\\n\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14914</th>\n",
       "      <td>def get_uuid(length=32, version=1):\\n    \"\"\"\\n...</td>\n",
       "      <td>Returns a unique ID of a given length.\\n    Us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14915</th>\n",
       "      <td>def get_unique_key_from_get(get_dict):\\n    \"\"...</td>\n",
       "      <td>Build a unique key from get data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14916</th>\n",
       "      <td>def get_domain(url):\\n    \"\"\" Returns domain n...</td>\n",
       "      <td>Returns domain name portion of a URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14917</th>\n",
       "      <td>def get_url_args(url):\\n    \"\"\" Returns a dict...</td>\n",
       "      <td>Returns a dictionary from a URL params</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14918 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    code  \\\n",
       "0      def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...   \n",
       "1      def dailymotion_download(url, output_dir='.', ...   \n",
       "2      def sina_download(url, output_dir='.', merge=T...   \n",
       "3      def sprint(text, *colors):\\n    \"\"\"Format text...   \n",
       "4      def print_log(text, *colors):\\n    \"\"\"Print a ...   \n",
       "...                                                  ...   \n",
       "14913  def from_grayscale(im, channels_on=(True, True...   \n",
       "14914  def get_uuid(length=32, version=1):\\n    \"\"\"\\n...   \n",
       "14915  def get_unique_key_from_get(get_dict):\\n    \"\"...   \n",
       "14916  def get_domain(url):\\n    \"\"\" Returns domain n...   \n",
       "14917  def get_url_args(url):\\n    \"\"\" Returns a dict...   \n",
       "\n",
       "                                               docstring  \n",
       "0      str->list\\n    Convert XML to URL List.\\n    F...  \n",
       "1                   Downloads Dailymotion videos by URL.  \n",
       "2                          Downloads Sina videos by URL.  \n",
       "3      Format text with color or other effects into A...  \n",
       "4                 Print a log message to standard error.  \n",
       "...                                                  ...  \n",
       "14913  Return a canvas from a grayscale image.\\n\\n   ...  \n",
       "14914  Returns a unique ID of a given length.\\n    Us...  \n",
       "14915                   Build a unique key from get data  \n",
       "14916               Returns domain name portion of a URL  \n",
       "14917             Returns a dictionary from a URL params  \n",
       "\n",
       "[14918 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"D:\\\\PROJECT\\\\out\\\\original\\\\small\\\\results.pkl\", 'rb') as f:\n",
    "    frame = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge2_precision': 0.0444,\n",
       " 'rouge2_recall': 0.0092,\n",
       " 'rouge2_fmeasure': 0.0132,\n",
       " 'bleu_score': 0.0018771960314820039,\n",
       " 'meteor_score': 0.0681162073537217}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_path = \"D:\\PROJECT\\out\\original\\small\\model_out\"\n",
    "medium_path = \"D:\\PROJECT\\out\\original\\medium\\model_out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_model = modelSetup(small_path)\n",
    "medium_model = modelSetup(medium_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tester = testWrapper(small_model)\n",
    "medium_tester = testWrapper(medium_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\PROJECT\\Level-4-Project\\notebooks\\evaluation.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000039?line=0'>1</a>\u001b[0m medium_per_res \u001b[39m=\u001b[39m frame_test\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x : medium_tester\u001b[39m.\u001b[39;49mgenerate_string(x), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\pandas\\core\\frame.py:8833\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/frame.py?line=8821'>8822</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/frame.py?line=8823'>8824</a>\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/frame.py?line=8824'>8825</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/frame.py?line=8825'>8826</a>\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/frame.py?line=8830'>8831</a>\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/frame.py?line=8831'>8832</a>\u001b[0m )\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/frame.py?line=8832'>8833</a>\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\pandas\\core\\apply.py:727\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=723'>724</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=724'>725</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=726'>727</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\pandas\\core\\apply.py:851\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=849'>850</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=850'>851</a>\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=852'>853</a>\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=853'>854</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\pandas\\core\\apply.py:867\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=863'>864</a>\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=864'>865</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=865'>866</a>\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=866'>867</a>\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=867'>868</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=868'>869</a>\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=869'>870</a>\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/pandas/core/apply.py?line=870'>871</a>\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32md:\\PROJECT\\Level-4-Project\\notebooks\\evaluation.ipynb Cell 18'\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000039?line=0'>1</a>\u001b[0m medium_per_res \u001b[39m=\u001b[39m frame_test\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x : medium_tester\u001b[39m.\u001b[39;49mgenerate_string(x), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32md:\\PROJECT\\Level-4-Project\\notebooks\\evaluation.ipynb Cell 6'\u001b[0m in \u001b[0;36mtestWrapper.generate_string\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000005?line=28'>29</a>\u001b[0m input_ids \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000005?line=29'>30</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mattention_mask\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000005?line=30'>31</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000005?line=31'>32</a>\u001b[0m output_str \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000005?line=32'>33</a>\u001b[0m batch[\u001b[39m\"\u001b[39m\u001b[39mpred_string\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output_str\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\generation_utils.py:1251\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1246'>1247</a>\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1247'>1248</a>\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1248'>1249</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1249'>1250</a>\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1250'>1251</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1251'>1252</a>\u001b[0m         input_ids,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1252'>1253</a>\u001b[0m         beam_scorer,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1253'>1254</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1254'>1255</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1255'>1256</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39mpad_token_id,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1256'>1257</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39meos_token_id,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1257'>1258</a>\u001b[0m         output_scores\u001b[39m=\u001b[39moutput_scores,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1258'>1259</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1259'>1260</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1260'>1261</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1261'>1262</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1263'>1264</a>\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1264'>1265</a>\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1265'>1266</a>\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1266'>1267</a>\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, typical_p\u001b[39m=\u001b[39mtypical_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1267'>1268</a>\u001b[0m     )\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\generation_utils.py:2035\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2030'>2031</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2032'>2033</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2034'>2035</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2035'>2036</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2036'>2037</a>\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2037'>2038</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2038'>2039</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2039'>2040</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2041'>2042</a>\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2042'>2043</a>\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:514\u001b[0m, in \u001b[0;36mEncoderDecoderModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=508'>509</a>\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=509'>510</a>\u001b[0m         labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=510'>511</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=512'>513</a>\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=513'>514</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=514'>515</a>\u001b[0m     input_ids\u001b[39m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=515'>516</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=516'>517</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=517'>518</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=518'>519</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39mdecoder_inputs_embeds,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=519'>520</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=520'>521</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=521'>522</a>\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=522'>523</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=523'>524</a>\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=524'>525</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_decoder,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=525'>526</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=527'>528</a>\u001b[0m \u001b[39m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=528'>529</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:973\u001b[0m, in \u001b[0;36mRobertaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=969'>970</a>\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=970'>971</a>\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=972'>973</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=973'>974</a>\u001b[0m     input_ids,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=974'>975</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=975'>976</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=976'>977</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=977'>978</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=978'>979</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=979'>980</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=980'>981</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=981'>982</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=982'>983</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=983'>984</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=984'>985</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=985'>986</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=986'>987</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=988'>989</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=989'>990</a>\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:850\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=840'>841</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=842'>843</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=843'>844</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=844'>845</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=847'>848</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=848'>849</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=849'>850</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=850'>851</a>\u001b[0m     embedding_output,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=851'>852</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=852'>853</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=853'>854</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=854'>855</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=855'>856</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=856'>857</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=857'>858</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=858'>859</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=859'>860</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=860'>861</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=861'>862</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=862'>863</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:526\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=516'>517</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=517'>518</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=518'>519</a>\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=522'>523</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=523'>524</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=524'>525</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=525'>526</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=526'>527</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=527'>528</a>\u001b[0m         attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=528'>529</a>\u001b[0m         layer_head_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=529'>530</a>\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=530'>531</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=531'>532</a>\u001b[0m         past_key_value,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=532'>533</a>\u001b[0m         output_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=533'>534</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=535'>536</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=536'>537</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:412\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=399'>400</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=400'>401</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=401'>402</a>\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=408'>409</a>\u001b[0m ):\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=409'>410</a>\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=410'>411</a>\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=411'>412</a>\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=412'>413</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=413'>414</a>\u001b[0m         attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=414'>415</a>\u001b[0m         head_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=415'>416</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=416'>417</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=417'>418</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=418'>419</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=420'>421</a>\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:339\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=328'>329</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=329'>330</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=330'>331</a>\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=336'>337</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=337'>338</a>\u001b[0m ):\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=338'>339</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=339'>340</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=340'>341</a>\u001b[0m         attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=341'>342</a>\u001b[0m         head_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=342'>343</a>\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=343'>344</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=344'>345</a>\u001b[0m         past_key_value,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=345'>346</a>\u001b[0m         output_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=346'>347</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=347'>348</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=348'>349</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:221\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=218'>219</a>\u001b[0m \u001b[39melif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=219'>220</a>\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=220'>221</a>\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(hidden_states))\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=221'>222</a>\u001b[0m     key_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m0\u001b[39m], key_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=222'>223</a>\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:190\u001b[0m, in \u001b[0;36mRobertaSelfAttention.transpose_for_scores\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=187'>188</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranspose_for_scores\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=188'>189</a>\u001b[0m     new_x_shape \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size)\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=189'>190</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(new_x_shape)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=190'>191</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "medium_per_res = frame_test.apply(lambda x : medium_tester.generate_string(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14918/14918 [43:07<00:00,  5.77ex/s]\n"
     ]
    }
   ],
   "source": [
    "small_per_res = test_set.map(small_tester.generate_string, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 558/14918 [17:58<7:42:35,  1.93s/ex] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\PROJECT\\Level-4-Project\\notebooks\\evaluation.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000045?line=0'>1</a>\u001b[0m medium_per_res \u001b[39m=\u001b[39m test_set\u001b[39m.\u001b[39;49mmap(medium_tester\u001b[39m.\u001b[39;49mgenerate_string, batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\datasets\\arrow_dataset.py:1953\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1949'>1950</a>\u001b[0m disable_tqdm \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1951'>1952</a>\u001b[0m \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m num_proc \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1952'>1953</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_single(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1953'>1954</a>\u001b[0m         function\u001b[39m=\u001b[39;49mfunction,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1954'>1955</a>\u001b[0m         with_indices\u001b[39m=\u001b[39;49mwith_indices,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1955'>1956</a>\u001b[0m         with_rank\u001b[39m=\u001b[39;49mwith_rank,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1956'>1957</a>\u001b[0m         input_columns\u001b[39m=\u001b[39;49minput_columns,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1957'>1958</a>\u001b[0m         batched\u001b[39m=\u001b[39;49mbatched,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1958'>1959</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1959'>1960</a>\u001b[0m         drop_last_batch\u001b[39m=\u001b[39;49mdrop_last_batch,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1960'>1961</a>\u001b[0m         remove_columns\u001b[39m=\u001b[39;49mremove_columns,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1961'>1962</a>\u001b[0m         keep_in_memory\u001b[39m=\u001b[39;49mkeep_in_memory,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1962'>1963</a>\u001b[0m         load_from_cache_file\u001b[39m=\u001b[39;49mload_from_cache_file,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1963'>1964</a>\u001b[0m         cache_file_name\u001b[39m=\u001b[39;49mcache_file_name,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1964'>1965</a>\u001b[0m         writer_batch_size\u001b[39m=\u001b[39;49mwriter_batch_size,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1965'>1966</a>\u001b[0m         features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1966'>1967</a>\u001b[0m         disable_nullable\u001b[39m=\u001b[39;49mdisable_nullable,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1967'>1968</a>\u001b[0m         fn_kwargs\u001b[39m=\u001b[39;49mfn_kwargs,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1968'>1969</a>\u001b[0m         new_fingerprint\u001b[39m=\u001b[39;49mnew_fingerprint,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1969'>1970</a>\u001b[0m         disable_tqdm\u001b[39m=\u001b[39;49mdisable_tqdm,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1970'>1971</a>\u001b[0m         desc\u001b[39m=\u001b[39;49mdesc,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1971'>1972</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1972'>1973</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1974'>1975</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\datasets\\arrow_dataset.py:519\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=516'>517</a>\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=517'>518</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=518'>519</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=519'>520</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=520'>521</a>\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m datasets:\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=521'>522</a>\u001b[0m     \u001b[39m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\datasets\\arrow_dataset.py:486\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=478'>479</a>\u001b[0m self_format \u001b[39m=\u001b[39m {\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=479'>480</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_type,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=480'>481</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mformat_kwargs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_kwargs,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=481'>482</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_columns,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=482'>483</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39moutput_all_columns\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_all_columns,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=483'>484</a>\u001b[0m }\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=484'>485</a>\u001b[0m \u001b[39m# apply actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=485'>486</a>\u001b[0m out: Union[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDatasetDict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=486'>487</a>\u001b[0m datasets: List[\u001b[39m\"\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(out\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m [out]\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=487'>488</a>\u001b[0m \u001b[39m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\datasets\\fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/fingerprint.py?line=451'>452</a>\u001b[0m             kwargs[fingerprint_name] \u001b[39m=\u001b[39m update_fingerprint(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/fingerprint.py?line=452'>453</a>\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/fingerprint.py?line=453'>454</a>\u001b[0m             )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/fingerprint.py?line=455'>456</a>\u001b[0m \u001b[39m# Call actual function\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/fingerprint.py?line=457'>458</a>\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/fingerprint.py?line=459'>460</a>\u001b[0m \u001b[39m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/fingerprint.py?line=461'>462</a>\u001b[0m \u001b[39mif\u001b[39;00m inplace:  \u001b[39m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\datasets\\arrow_dataset.py:2318\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=2315'>2316</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched:\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=2316'>2317</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, example \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(pbar):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=2317'>2318</a>\u001b[0m         example \u001b[39m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[39m=\u001b[39;49moffset)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=2318'>2319</a>\u001b[0m         \u001b[39mif\u001b[39;00m update_data:\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=2319'>2320</a>\u001b[0m             \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\datasets\\arrow_dataset.py:2218\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=2215'>2216</a>\u001b[0m \u001b[39mif\u001b[39;00m with_rank:\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=2216'>2217</a>\u001b[0m     additional_args \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (rank,)\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=2217'>2218</a>\u001b[0m processed_inputs \u001b[39m=\u001b[39m function(\u001b[39m*\u001b[39mfn_args, \u001b[39m*\u001b[39madditional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_kwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=2218'>2219</a>\u001b[0m \u001b[39mif\u001b[39;00m update_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=2219'>2220</a>\u001b[0m     \u001b[39m# Check if the function returns updated examples\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=2220'>2221</a>\u001b[0m     update_data \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[39m.\u001b[39mTable))\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\datasets\\arrow_dataset.py:1913\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[1;34m(item, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1908'>1909</a>\u001b[0m decorated_item \u001b[39m=\u001b[39m (\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1909'>1910</a>\u001b[0m     Example(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m batched \u001b[39melse\u001b[39;00m Batch(item, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1910'>1911</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1911'>1912</a>\u001b[0m \u001b[39m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1912'>1913</a>\u001b[0m result \u001b[39m=\u001b[39m f(decorated_item, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1913'>1914</a>\u001b[0m \u001b[39m# Return a standard dict\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/datasets/arrow_dataset.py?line=1914'>1915</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mdata \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result, LazyDict) \u001b[39melse\u001b[39;00m result\n",
      "\u001b[1;32md:\\PROJECT\\Level-4-Project\\notebooks\\evaluation.ipynb Cell 6'\u001b[0m in \u001b[0;36mtestWrapper.generate_string\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000005?line=28'>29</a>\u001b[0m input_ids \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000005?line=29'>30</a>\u001b[0m attention_mask \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mattention_mask\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000005?line=30'>31</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000005?line=31'>32</a>\u001b[0m output_str \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/evaluation.ipynb#ch0000005?line=32'>33</a>\u001b[0m batch[\u001b[39m\"\u001b[39m\u001b[39mpred_string\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m output_str\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\generation_utils.py:1251\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1246'>1247</a>\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1247'>1248</a>\u001b[0m         input_ids, expand_size\u001b[39m=\u001b[39mnum_beams, is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1248'>1249</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1249'>1250</a>\u001b[0m     \u001b[39m# 12. run beam search\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1250'>1251</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeam_search(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1251'>1252</a>\u001b[0m         input_ids,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1252'>1253</a>\u001b[0m         beam_scorer,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1253'>1254</a>\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1254'>1255</a>\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1255'>1256</a>\u001b[0m         pad_token_id\u001b[39m=\u001b[39mpad_token_id,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1256'>1257</a>\u001b[0m         eos_token_id\u001b[39m=\u001b[39meos_token_id,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1257'>1258</a>\u001b[0m         output_scores\u001b[39m=\u001b[39moutput_scores,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1258'>1259</a>\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1259'>1260</a>\u001b[0m         synced_gpus\u001b[39m=\u001b[39msynced_gpus,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1260'>1261</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1261'>1262</a>\u001b[0m     )\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1263'>1264</a>\u001b[0m \u001b[39melif\u001b[39;00m is_beam_sample_gen_mode:\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1264'>1265</a>\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1265'>1266</a>\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1266'>1267</a>\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, typical_p\u001b[39m=\u001b[39mtypical_p, temperature\u001b[39m=\u001b[39mtemperature, num_beams\u001b[39m=\u001b[39mnum_beams\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=1267'>1268</a>\u001b[0m     )\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\generation_utils.py:2035\u001b[0m, in \u001b[0;36mGenerationMixin.beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2030'>2031</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2032'>2033</a>\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2034'>2035</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2035'>2036</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2036'>2037</a>\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2037'>2038</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2038'>2039</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2039'>2040</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2041'>2042</a>\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/generation_utils.py?line=2042'>2043</a>\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:514\u001b[0m, in \u001b[0;36mEncoderDecoderModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=508'>509</a>\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=509'>510</a>\u001b[0m         labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=510'>511</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=512'>513</a>\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=513'>514</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=514'>515</a>\u001b[0m     input_ids\u001b[39m=\u001b[39mdecoder_input_ids,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=515'>516</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39mdecoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=516'>517</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=517'>518</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=518'>519</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39mdecoder_inputs_embeds,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=519'>520</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=520'>521</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=521'>522</a>\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=522'>523</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39mpast_key_values,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=523'>524</a>\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=524'>525</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs_decoder,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=525'>526</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=527'>528</a>\u001b[0m \u001b[39m# Compute loss independent from decoder (as some shift the logits inside them)\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py?line=528'>529</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:973\u001b[0m, in \u001b[0;36mRobertaForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=969'>970</a>\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=970'>971</a>\u001b[0m     use_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=972'>973</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=973'>974</a>\u001b[0m     input_ids,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=974'>975</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=975'>976</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=976'>977</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=977'>978</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=978'>979</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=979'>980</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=980'>981</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=981'>982</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=982'>983</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=983'>984</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=984'>985</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=985'>986</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=986'>987</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=988'>989</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=989'>990</a>\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:850\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=840'>841</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=842'>843</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=843'>844</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=844'>845</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=847'>848</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=848'>849</a>\u001b[0m )\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=849'>850</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=850'>851</a>\u001b[0m     embedding_output,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=851'>852</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=852'>853</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=853'>854</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=854'>855</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=855'>856</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=856'>857</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=857'>858</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=858'>859</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=859'>860</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=860'>861</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=861'>862</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=862'>863</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:526\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=516'>517</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=517'>518</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=518'>519</a>\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=522'>523</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=523'>524</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=524'>525</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=525'>526</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=526'>527</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=527'>528</a>\u001b[0m         attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=528'>529</a>\u001b[0m         layer_head_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=529'>530</a>\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=530'>531</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=531'>532</a>\u001b[0m         past_key_value,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=532'>533</a>\u001b[0m         output_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=533'>534</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=535'>536</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=536'>537</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:412\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=399'>400</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=400'>401</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=401'>402</a>\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=408'>409</a>\u001b[0m ):\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=409'>410</a>\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=410'>411</a>\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=411'>412</a>\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=412'>413</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=413'>414</a>\u001b[0m         attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=414'>415</a>\u001b[0m         head_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=415'>416</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=416'>417</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=417'>418</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=418'>419</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=420'>421</a>\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:339\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=328'>329</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=329'>330</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=330'>331</a>\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=336'>337</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=337'>338</a>\u001b[0m ):\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=338'>339</a>\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=339'>340</a>\u001b[0m         hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=340'>341</a>\u001b[0m         attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=341'>342</a>\u001b[0m         head_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=342'>343</a>\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=343'>344</a>\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=344'>345</a>\u001b[0m         past_key_value,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=345'>346</a>\u001b[0m         output_attentions,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=346'>347</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=347'>348</a>\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=348'>349</a>\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:221\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=218'>219</a>\u001b[0m \u001b[39melif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=219'>220</a>\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=220'>221</a>\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(hidden_states))\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=221'>222</a>\u001b[0m     key_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m0\u001b[39m], key_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=222'>223</a>\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\build\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:190\u001b[0m, in \u001b[0;36mRobertaSelfAttention.transpose_for_scores\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=187'>188</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranspose_for_scores\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=188'>189</a>\u001b[0m     new_x_shape \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size)\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=189'>190</a>\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(new_x_shape)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/build/lib/site-packages/transformers/models/roberta/modeling_roberta.py?line=190'>191</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "medium_per_res = test_set.map(medium_tester.generate_string, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "small_per_scores = eval_compute(small_res)\n",
    "medium_per_scores = eval_compute(medium_res)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge2_precision': 0.9949,\n",
       " 'rouge2_recall': 0.5738,\n",
       " 'rouge2_fmeasure': 0.6461,\n",
       " 'bleu_score': 1.1251101518020312,\n",
       " 'meteor_score': 0.5604892748778642}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#medium_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge2_precision': 0.0444,\n",
       " 'rouge2_recall': 0.0092,\n",
       " 'rouge2_fmeasure': 0.0132,\n",
       " 'bleu_score': 0.0018771960314820039,\n",
       " 'meteor_score': 0.0681162073537217}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#small_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The test function to be used for training.\\n\\n    Args:\\n ']\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer([str('import tensorflow as tf mnist = tf.keras.datasets.mnist (x_train, y_train),(x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0')], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids.cuda()\n",
    "attention_mask = inputs.attention_mask.cuda()\n",
    "outputs = medium_model.generate(input_ids, attention_mask=attention_mask)\n",
    "output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_path = \"\"\n",
    "medium_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "043b8ec81c094910c2b30aa6da4b87ee4064e49c6c2745b34a15c1b8c91b4ed0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('build')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
