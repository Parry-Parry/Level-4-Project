{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Training\n",
    "\n",
    "* Custom Tokenizer\n",
    "* GRU Encoder / Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import ijson\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "sys.path.append('D:\\PROJECT\\Level-4-Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code2text.models.baseline.model import seq2seqTrain, MaskedLoss\n",
    "from code2text.helper.model import BatchLogs\n",
    "from code2text.helper.preprocess import tf_lower_and_split_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "* Load Data\n",
    "* Format to pandas\n",
    "* Format into single file for each language \n",
    "* tf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\PROJECT\\data\\CodeSearchNet\"\n",
    "langs = [\"go\", \"java\", \"python\", \"javascript\", \"ruby\", \"php\"]\n",
    "format = [\"train.jsonl\", \"test.jsonl\", \"valid.jsonl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, encoding=\"UTF-8\") as json_file:\n",
    "        cursor = 0\n",
    "        code_set = []\n",
    "        string_set = []\n",
    "        lang_set = []\n",
    "        for _, line in enumerate(json_file):\n",
    "            code = None\n",
    "            string = None\n",
    "            line_as_file = io.StringIO(line)\n",
    "            json_parser = ijson.parse(line_as_file)\n",
    "\n",
    "            for prefix, _, value in json_parser:\n",
    "                if prefix == \"code\":  \n",
    "                    code = value\n",
    "                if prefix == \"docstring\":\n",
    "                    string = value\n",
    "                if prefix == \"language\":\n",
    "                    lang = value\n",
    "            if (code is not None and string is not None):\n",
    "                code_set.append(code)\n",
    "                string_set.append(string)\n",
    "                lang_set.append(lang)\n",
    "\n",
    "            cursor += len(line)\n",
    "    return pd.DataFrame(data={'code': code_set, 'docstring': string_set, 'language': lang_set})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing  go   train.jsonl ... \n",
      "Processing  go   test.jsonl ... \n",
      "Processing  go   valid.jsonl ... \n",
      "Processing  java   train.jsonl ... \n",
      "Processing  java   test.jsonl ... \n",
      "Processing  java   valid.jsonl ... \n",
      "Processing  python   train.jsonl ... \n",
      "Processing  python   test.jsonl ... \n",
      "Processing  python   valid.jsonl ... \n",
      "Processing  javascript   train.jsonl ... \n",
      "Processing  javascript   test.jsonl ... \n",
      "Processing  javascript   valid.jsonl ... \n",
      "Processing  ruby   train.jsonl ... \n",
      "Processing  ruby   test.jsonl ... \n",
      "Processing  ruby   valid.jsonl ... \n",
      "Processing  php   train.jsonl ... \n",
      "Processing  php   test.jsonl ... \n",
      "Processing  php   valid.jsonl ... \n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "test = []\n",
    "valid = []\n",
    "\n",
    "for lang in langs:\n",
    "    tmp = os.path.join(path, lang)\n",
    "    for file in format:\n",
    "        print(\"Processing \", lang, \" \", file, \"... \")\n",
    "        data = read_data(os.path.join(tmp, file))\n",
    "        if file == \"train.jsonl\":\n",
    "            train.append(data)\n",
    "        if file == \"test.jsonl\":\n",
    "            test.append(data)\n",
    "        if file == \"valid.jsonl\":\n",
    "            valid.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat(train)\n",
    "test = pd.concat(test)\n",
    "valid = pd.concat(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test, valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.to_json(\"D:\\PROJECT\\data\\CodeSearchNet\\Combine_clean\\data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"D:\\PROJECT\\data\\CodeSearchNet\\Combine_clean\\data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(data[\"code\"].values, tf.string),\n",
    "            tf.cast(data[\"docstring\"].values, tf.string)\n",
    "        )\n",
    "    )\n",
    ").shuffle(len(data[\"code\"])).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(train[\"code\"].values, tf.string),\n",
    "            tf.cast(train[\"docstring\"].values, tf.string)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "test_set = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(test[\"code\"].values, tf.string),\n",
    "            tf.cast(test[\"docstring\"].values, tf.string)\n",
    "        )\n",
    "    )\n",
    ")\n",
    "valid_set = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(valid[\"code\"].values, tf.string),\n",
    "            tf.cast(valid[\"docstring\"].values, tf.string)\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config & Build\n",
    "\n",
    "**TODO:**\n",
    "\n",
    "* Summarise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_processor = input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=50000)\n",
    "\n",
    "output_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_processor.adapt(data[\"code\"])\n",
    "output_processor.adapt(data[\"docstring\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq2seqTrain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\PROJECT\\Level-4-Project\\notebooks\\training\\baseline.ipynb Cell 18'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/training/baseline.ipynb#ch0000016?line=0'>1</a>\u001b[0m train_model \u001b[39m=\u001b[39m seq2seqTrain(\u001b[39m100\u001b[39m, \u001b[39m32\u001b[39m, input_text_processor\u001b[39m=\u001b[39minput_processor,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/training/baseline.ipynb#ch0000016?line=1'>2</a>\u001b[0m     output_text_processor\u001b[39m=\u001b[39moutput_processor)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'seq2seqTrain' is not defined"
     ]
    }
   ],
   "source": [
    "train_model = seq2seqTrain(100, 32, input_text_processor=input_processor,\n",
    "    output_text_processor=output_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Parry\\AppData\\Local\\Temp\\ipykernel_20084\\2871295278.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3/15711 [..............................] - ETA: 186:37:04 - batch_loss: 10.4694"
     ]
    }
   ],
   "source": [
    "train_model.fit(dataset, epochs=1, callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss.logs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
