{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model Training\n",
    "\n",
    "* Custom Tokenizer\n",
    "* GRU Encoder / Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "sys.path.append('D:\\PROJECT\\Level-4-Project')\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code2text.models.baseline.model import seq2seqTrain, MaskedLoss\n",
    "from code2text.helper.model import BatchLogs\n",
    "from code2text.helper.preprocess import tf_lower_and_split_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.debugging.set_log_device_placement(True)\n",
    "tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)\n",
    "#tf.config.experimental.set_virtual_device_configuration(tf.config.experimental.list_physical_devices('GPU')[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\PROJECT\\data\\CodeSearchNet\"\n",
    "langs = [\"go\", \"java\", \"python\", \"javascript\", \"ruby\", \"php\"]\n",
    "format = [\"train.jsonl\", \"test.jsonl\", \"valid.jsonl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with open(path, encoding=\"UTF-8\") as json_file:\n",
    "        cursor = 0\n",
    "        code_set = []\n",
    "        string_set = []\n",
    "        lang_set = []\n",
    "        for _, line in enumerate(json_file):\n",
    "            code = None\n",
    "            string = None\n",
    "            line_as_file = io.StringIO(line)\n",
    "            json_parser = ijson.parse(line_as_file)\n",
    "\n",
    "            for prefix, _, value in json_parser:\n",
    "                if prefix == \"code\":  \n",
    "                    code = value\n",
    "                if prefix == \"docstring\":\n",
    "                    string = value\n",
    "                if prefix == \"language\":\n",
    "                    lang = value\n",
    "            if (code is not None and string is not None):\n",
    "                code_set.append(code)\n",
    "                string_set.append(string)\n",
    "                lang_set.append(lang)\n",
    "\n",
    "            cursor += len(line)\n",
    "    return pd.DataFrame(data={'code': code_set, 'docstring': string_set, 'language': lang_set})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    "valid = []\n",
    "\n",
    "for lang in langs:\n",
    "    tmp = os.path.join(path, lang)\n",
    "    for file in format:\n",
    "        print(\"Processing \", lang, \" \", file, \"... \")\n",
    "        data = read_data(os.path.join(tmp, file))\n",
    "        if file == \"train.jsonl\":\n",
    "            train.append(data)\n",
    "        if file == \"test.jsonl\":\n",
    "            test.append(data)\n",
    "        if file == \"valid.jsonl\":\n",
    "            valid.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat(train)\n",
    "test = pd.concat(test)\n",
    "valid = pd.concat(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test, valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True)\n",
    "train.to_json(\"D:\\PROJECT\\data\\CodeSearchNet\\Combine_clean\\\\train.json\")\n",
    "test.reset_index(inplace=True)\n",
    "test.to_json(\"D:\\PROJECT\\data\\CodeSearchNet\\Combine_clean\\\\test.json\")\n",
    "valid.reset_index(inplace=True)\n",
    "valid.to_json(\"D:\\PROJECT\\data\\CodeSearchNet\\Combine_clean\\\\valid.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data.to_json(\"D:\\PROJECT\\data\\CodeSearchNet\\Combine_clean\\data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\"D:\\PROJECT\\data\\CodeSearchNet\\Combine_clean\\data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json(\"D:\\PROJECT\\data\\CodeSearchNet\\Combine_clean\\\\train.json\")\n",
    "valid = pd.read_json(\"D:\\PROJECT\\data\\CodeSearchNet\\Combine_clean\\\\valid.json\")\n",
    "test = pd.read_json(\"D:\\PROJECT\\data\\CodeSearchNet\\Combine_clean\\\\test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "buffer = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(data[\"code\"].values, tf.string),\n",
    "            tf.cast(data[\"docstring\"].values, tf.string)\n",
    "        )\n",
    "    )\n",
    ").shuffle(buffer).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    train_set = (\n",
    "        tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                tf.cast(train[\"code\"].values, tf.string),\n",
    "                tf.cast(train[\"docstring\"].values, tf.string)\n",
    "            )\n",
    "        )\n",
    "    ).shuffle(buffer).batch(batch_size, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    test_set = (\n",
    "        tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                tf.cast(test[\"code\"].values, tf.string),\n",
    "                tf.cast(test[\"docstring\"].values, tf.string)\n",
    "            )\n",
    "        )\n",
    "    ).shuffle(buffer).batch(batch_size, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    valid_set = (\n",
    "        tf.data.Dataset.from_tensor_slices(\n",
    "            (\n",
    "                tf.cast(valid[\"code\"].values, tf.string),\n",
    "                tf.cast(valid[\"docstring\"].values, tf.string)\n",
    "            )  \n",
    "        )\n",
    "    ).shuffle(buffer).batch(batch_size, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config & Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = 40000\n",
    "input_processor = input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=tokens)\n",
    "\n",
    "output_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_processor.adapt(train[\"code\"])\n",
    "output_processor.adapt(train[\"docstring\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = seq2seqTrain(112, 48, input_text_processor=input_processor,\n",
    "    output_text_processor=output_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.compile(\n",
    "    optimizer=tf.optimizers.Adam(learning_rate=0.0004),\n",
    "    loss=MaskedLoss(),\n",
    "    metrics=['acc', text.metrics.rouge_l]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss = BatchLogs('batch_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Parry\\AppData\\Local\\Temp\\ipykernel_13832\\2871295278.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 345/1642 [=====>........................] - ETA: 1:21:31 - batch_loss: 6.8736"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:GPU:0;4813655c55b4b0f0;/job:localhost/replica:0/task:0/device:GPU:0;edge_3336_StatefulPartitionedCall/while/body/_59/while/gradient_tape/while/gradients/while/decoder/embedding_1/embedding_lookup_grad/Size;15032662501523675017:428\n\t [[{{node StatefulPartitionedCall/while/body/_59/while/gradient_tape/while/gradients/while/decoder/embedding_1/embedding_lookup_grad/Size/_244}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[StatefulPartitionedCall/while/LoopCond/_276/_330]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) Resource exhausted:  SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:GPU:0;4813655c55b4b0f0;/job:localhost/replica:0/task:0/device:GPU:0;edge_3336_StatefulPartitionedCall/while/body/_59/while/gradient_tape/while/gradients/while/decoder/embedding_1/embedding_lookup_grad/Size;15032662501523675017:428\n\t [[{{node StatefulPartitionedCall/while/body/_59/while/gradient_tape/while/gradients/while/decoder/embedding_1/embedding_lookup_grad/Size/_244}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_8931]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32md:\\PROJECT\\Level-4-Project\\notebooks\\training\\baseline.ipynb Cell 32'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/PROJECT/Level-4-Project/notebooks/training/baseline.ipynb#ch0000030?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m train_model\u001b[39m.\u001b[39;49mfit(test_set, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mvalid_set, callbacks\u001b[39m=\u001b[39;49m[batch_loss])\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/keras/engine/training.py?line=1176'>1177</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/keras/engine/training.py?line=1177'>1178</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/keras/engine/training.py?line=1178'>1179</a>\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/keras/engine/training.py?line=1179'>1180</a>\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/keras/engine/training.py?line=1180'>1181</a>\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/keras/engine/training.py?line=1181'>1182</a>\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/keras/engine/training.py?line=1182'>1183</a>\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/keras/engine/training.py?line=1183'>1184</a>\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/keras/engine/training.py?line=1184'>1185</a>\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/keras/engine/training.py?line=1185'>1186</a>\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=881'>882</a>\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=883'>884</a>\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=884'>885</a>\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=886'>887</a>\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=887'>888</a>\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:924\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=920'>921</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=921'>922</a>\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=922'>923</a>\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=923'>924</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=924'>925</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=925'>926</a>\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/def_function.py?line=926'>927</a>\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=3035'>3036</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=3036'>3037</a>\u001b[0m   (graph_function,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=3037'>3038</a>\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=3038'>3039</a>\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=3039'>3040</a>\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=1958'>1959</a>\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=1959'>1960</a>\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=1960'>1961</a>\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=1961'>1962</a>\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=1962'>1963</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=1963'>1964</a>\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=1964'>1965</a>\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=1965'>1966</a>\u001b[0m     args,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=1966'>1967</a>\u001b[0m     possible_gradient_type,\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=1967'>1968</a>\u001b[0m     executing_eagerly)\n\u001b[0;32m   <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=1968'>1969</a>\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=588'>589</a>\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=589'>590</a>\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=590'>591</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=591'>592</a>\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=592'>593</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=593'>594</a>\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=594'>595</a>\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=595'>596</a>\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=596'>597</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=597'>598</a>\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=598'>599</a>\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=599'>600</a>\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=602'>603</a>\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/function.py?line=603'>604</a>\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mD:\\CONDA\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/execute.py?line=56'>57</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/execute.py?line=57'>58</a>\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/execute.py?line=58'>59</a>\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/execute.py?line=59'>60</a>\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/execute.py?line=60'>61</a>\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     <a href='file:///d%3A/CONDA/envs/tfgpu/lib/site-packages/tensorflow/python/eager/execute.py?line=61'>62</a>\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:GPU:0;4813655c55b4b0f0;/job:localhost/replica:0/task:0/device:GPU:0;edge_3336_StatefulPartitionedCall/while/body/_59/while/gradient_tape/while/gradients/while/decoder/embedding_1/embedding_lookup_grad/Size;15032662501523675017:428\n\t [[{{node StatefulPartitionedCall/while/body/_59/while/gradient_tape/while/gradients/while/decoder/embedding_1/embedding_lookup_grad/Size/_244}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n\t [[StatefulPartitionedCall/while/LoopCond/_276/_330]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n  (1) Resource exhausted:  SameWorkerRecvDone unable to allocate output tensor. Key: /job:localhost/replica:0/task:0/device:GPU:0;4813655c55b4b0f0;/job:localhost/replica:0/task:0/device:GPU:0;edge_3336_StatefulPartitionedCall/while/body/_59/while/gradient_tape/while/gradients/while/decoder/embedding_1/embedding_lookup_grad/Size;15032662501523675017:428\n\t [[{{node StatefulPartitionedCall/while/body/_59/while/gradient_tape/while/gradients/while/decoder/embedding_1/embedding_lookup_grad/Size/_244}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_8931]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "history = train_model.fit(train_set, epochs=3, validation_data=valid_set, callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
